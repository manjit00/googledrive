{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP119Sp6iShmaXQ66VIDjTp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# BPE is Byte Pair Tokenization\n","# word based tokenization is not really possible because there could billions\n","# of combinations\n","# however, the character based token, there are only 256 distinct.\n","# the character level losses the meaning of the word.\n","\n","# BPE is sub word tokenziation, best of both world, word and characters based.\n","# e.g boys, will be divided into boy and s. so the meanful breaking.\n","\n","# 1. Tokenization (word base, chararacter base, mostly BPE is used)\n","# 2. Vector Embeddings\n","# 3. Positional Embeddings\n","# 4. Input Embeddings = Vector + Postional - Context Embeddings.\n","# 5. Input Embeddings is input to neural networks.\n","\n","\n","import torch\n","import re, collections\n","import importlib.metadata\n","import tiktoken\n","from torch.utils.data import Dataset, DataLoader\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","with open('/content/drive/My Drive/Colab Notebooks/the-verdict.txt',\n","         'r', encoding='utf-8') as f:\n","  raw_text = f.read()\n","\n","print (raw_text[:99])\n","\n","GPT_CONFIG_124M = {\n","    \"emb_dim\": 256, #embedding dimenstion\n","    \"n_heads\": 8, #number of attention heads\n","    \"n_layers\": 12, #number of transformer layers\n","    \"context_length\": 8, #how many token process in any given time\n","    \"vocab_size\": 50257, # vocabalary size\n","    \"drop_rate\": 0.1,\n","    \"qkv_bias\": False\n","}\n","\n","\n","\n","#raw_text = \"Your journey start with one step\"\n","# this is STEP1 & STEP2\n","class GPTDataset(Dataset):\n","\n","  def __init__(self, txt, tokenizer, max_length, stride):\n","    self.input_ids = []\n","    self.target_ids = []\n","\n","    #Tokenize the entire text\n","    token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|?}\"})\n","\n","    # use the sliding winder to check the book\n","    for i in range(0, len(token_ids) - max_length, stride):\n","      input_chunks = token_ids[i:i+max_length]\n","      target_chunks = token_ids[i+1:i+1+max_length]\n","      self.input_ids.append(torch.tensor(input_chunks, dtype=torch.long))\n","      self.target_ids.append(torch.tensor(target_chunks, dtype=torch.long))\n","  def __len__(self):\n","    return len(self.input_ids)\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.target_ids[idx]\n","\n","\n","def create_dataloader (dataset, batch_size=8, max_length=256, stride=128,\n","                       shuffle=True, last_drop=True, num_workers=0):\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","    dataset = GPTDataset(dataset, tokenizer, max_length, stride)\n","    dataloader = DataLoader (dataset, batch_size, shuffle=False,\n","                            num_workers=0)\n","    return dataloader\n","\n","\n","# 1. Initialize the tokenizer\n","# 2. Create the dataset\n","# 3. drop_last=true drop the last batch.\n","# 4. number CPUs.\n","\n","print (\"PyTorch verson:\", torch.__version__)\n","dataloader = create_dataloader (\n","    raw_text,\n","    batch_size=2,\n","    max_length=8,\n","    stride=1,\n","    shuffle=False)\n","\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","#print (len(dataloader))\n","\n","# train/validation data\n","train_ratio = 0.9\n","split_idx = int(train_ratio * len(raw_text))\n","train_data = raw_text[:split_idx]\n","val_data = raw_text[split_idx:]\n","\n","torch.manual_seed(123)\n","\n","train_dataloader = create_dataloader (\n","    train_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    last_drop=True,\n","    shuffle=True,\n","    num_workers=0)\n","\n","val_dataloader = create_dataloader (\n","    val_data,\n","    batch_size=2,\n","    max_length=GPT_CONFIG_124M[\"context_length\"],\n","    stride=GPT_CONFIG_124M[\"context_length\"],\n","    last_drop=False,\n","    shuffle=False,\n","    num_workers=0)\n","\n","print (\"Train loader:\")\n","for x, y in train_dataloader:\n","  print (x.shape, y.shape)\n","print (\"Val loader:\")\n","for x, y in val_dataloader:\n","  print (x.shape, y.shape)\n","\n","print (len(train_dataloader))\n","print (len(val_dataloader))\n","\n","vocab_size = 50257\n","output_dim = 256\n","\n","#vector or token embeddings\n","token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n","token_embedding = token_embedding_layer(inputs)\n","print(\"Token Embedding Size\", token_embedding.shape)\n","\n","#positional embeddings\n","context_length = max_length = 8\n","position_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n","position_embedding = position_embedding_layer(torch.arange(max_length))\n","print(\"Position Embedding Size:\", position_embedding.shape)\n","\n","#input embedding = vector embedding + positional embedding.\n","input_embedding = token_embedding + position_embedding\n","print(\"Input Embedding Shape:\", input_embedding.shape)\n","\n","\n","# self attentions.\n","# 1. Query, Key, Value\n","# 2. Q = vector @ Query, K = vector @ keys, V = vector @ value\n","# 3. Attention score = Q * K^t\n","# 4. Attention normalized = Attention score/sqrt(key embedding dim)\n","# 5. Softmax (Attention normalized).\n","# 6. Context Vector = Softmax Attention * V\n","\n","# causal attentions. (ensure only factor current or previous token)\n","# 1. use of tril or triu\n","# 2. here you will set the upper triangle not zero but -inf.\n","# 3. after setting -inf, you can use the softmax. remaining should sum to 1.\n","# 4. dropout\n","\n","\n","d_in = 256\n","d_out = 256\n","dropout = 0.5\n","\n","print (\"Input Shape is\", inputs.shape)\n","\n","\n","# multiple head = causaul attn * many\n","class AttentionIsAllYouNeed(torch.nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout):\n","    super().__init__()\n","    self.dropout = torch.nn.Dropout(0.5)\n","    self.W_query = torch.nn.Linear(d_in, d_out, bias=False)\n","    self.W_key = torch.nn.Linear(d_in, d_out, bias=False)\n","    self.W_value = torch.nn.Linear(d_in, d_out, bias=False)\n","    self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","  def forward(self, X):\n","    b, num_tokens, d_in = X.shape\n","    print (\"X shape is\", X.shape)\n","\n","    keys = self.W_key(X)\n","    queries = self.W_query(X)\n","    values = self.W_value(X)\n","    print (\"keys shape is\", keys.shape)\n","\n","    attn_scores = queries @ keys.transpose(1,2) # 6x2 @ 2x6 = 6x6 matrix\n","\n","    attn_scores = attn_scores.masked_fill_(\n","        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) # num_token incase the\n","        # the size context is less than context size\n","    attn_weight = torch.softmax(attn_scores / (d_out ** 0.5), dim=-1)\n","    attn_weight = self.dropout(attn_weight)\n","    context_vector = attn_weight @ values # 6x6 @ 6x2 = 6x2\n","    return context_vector\n","\n","class MultiheadAttentionWrapper(torch.nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout, num_heads):\n","    super().__init__()\n","    self.heads = torch.nn.ModuleList(\n","        [AttentionIsAllYouNeed(d_in, d_out, context_length, dropout) for _ in range(num_heads)]\n","    )\n","  def forward(self, X):\n","    return torch.cat([head(X) for head in self.heads], dim=-1)\n","\n","\n","#input1 = 6x3\n","torch.manual_seed(789)\n","context_length = 8\n","# Test MultiheadAttentionWrapper\n","mha = MultiheadAttentionWrapper(d_in, d_out, context_length, dropout=0.5, num_heads=2)\n","mha_vec = mha(token_embedding)\n","\n","\n","# copied from vasuria used in chatgpt2.\n","class MultiheadAttention (torch.nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout, num_heads):\n","    super().__init__()\n","    print (\"d_out\", d_out)\n","    print (\"num_heads\", num_heads)\n","    assert (d_out % num_heads == 0), \\\n","      \"d_out must be divible by num_heads\"\n","    self.d_out = d_out\n","    self.num_heads = num_heads\n","    #STEP2 - define d_out and number of heads\n","    # we have decided to use d_out = 6, and num_heads = 2\n","    self.head_dim = d_out // num_heads\n","    self.W_query = torch.nn.Linear(d_in, d_out, bias=False) # this is 6x6\n","    self.W_key = torch.nn.Linear(d_in, d_out, bias=False) # this is 6x6\n","    self.W_value = torch.nn.Linear(d_in, d_out, bias=False) # this is 6x6\n","    self.out_proj = torch.nn.Linear(d_out, d_out)\n","    self.dropout = torch.nn.Dropout(dropout)\n","    self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n","\n","  def forward(self, X):\n","    b, num_tokens, d_in = X.shape #STEP1 - 1,3,6\n","    queries = self.W_query(X)\n","    keys = self.W_key(X)\n","    values = self.W_value(X)\n","\n","    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","    # transpose\n","    keys = keys.transpose(1,2)\n","    values = values.transpose(1,2)\n","    queries = queries.transpose(1,2)\n","\n","    attn_scores = queries @ keys.transpose(2,3)\n","\n","    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","    attn_scores = attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","    attn_weight = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n","    attn_weight = self.dropout(attn_weight)\n","    context_vector = attn_weight @ values\n","\n","    context_vector = context_vector.transpose(1,2)\n","    context_vector = context_vector.reshape(b, num_tokens, self.d_out)\n","    context_vector = self.out_proj(context_vector)\n","    return context_vector\n","\n","torch.manual_seed(123)\n","# Test MultiheadAttention\n","# Use a tensor with shape (batch_size, num_tokens, d_in) for testing\n","batch_for_mha = torch.randn(2, 8, 256) # Example shape, adjust as needed\n","mha2 = MultiheadAttention(256, 256, 8, dropout=0.0, num_heads=2)\n","mha2_vec = mha2(batch_for_mha)\n","\n","\n","# LLM architecture\n","# Transformer block\n","# 1. LayerNormalization\n","# 2. Multi-head attention\n","# 3. Dropout\n","# 4. shortcut (+)\n","# 5. LayerNormalization again\n","# 6. Feed forward\n","#     1. linear layer\n","#     2. GELU activation\n","#     3. Linear layer\n","# 7. Dropout\n","# 8. shortcut (+)\n","# 9. finally output layers\n","\n","\n","# Transformers.\n","# transformers have layers, meaning it has that many transformer layers.\n","\n","\n","\n","# STEP 1.\n","\n","import torch.nn as nn\n","\n","class GPTModel(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n","    self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n","    self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n","    self.trf_blocks = nn.Sequential(\n","        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])])\n","    self.final_norm = LayerNorm(config[\"emb_dim\"])\n","    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n","\n","  def forward(self, x):\n","    batch_size, context_length = x.shape\n","    tok_emb = self.tok_emb(x)\n","    pos_emb = self.pos_emb(torch.arange(context_length, device=x.device))\n","    x = tok_emb + pos_emb\n","    x = self.drop_emb(x)\n","    x = self.trf_blocks(x) # transformer block\n","    x = self.final_norm(x)\n","    logits = self.out_head(x)\n","    return logits\n","\n","\n","class TransformerBlock(nn.Module):\n","  def __init__(self, config):\n","    super().__init__()\n","    self.attn = MultiheadAttention(config[\"emb_dim\"], config[\"emb_dim\"],\n","                                   config[\"context_length\"], config[\"drop_rate\"],\n","                                   config[\"n_heads\"])\n","    self.ff = FeedForward(config[\"emb_dim\"], config[\"drop_rate\"])\n","    self.ln1 = LayerNorm(config[\"emb_dim\"])\n","    self.ln2 = LayerNorm(config[\"emb_dim\"])\n","    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n","\n","\n","  def forward(self, x):\n","    shortcut = x\n","    x = self.ln1(x)\n","    x = self.attn(x)\n","    x = self.drop_shortcut(x)\n","    x = shortcut + x\n","    shortcut = x\n","    x = self.ln2(x)\n","    x = self.ff(x)\n","    x = self.drop_shortcut(x)\n","    x = shortcut + x\n","    return x\n","\n","class LayerNorm(nn.Module):\n","  def __init__(self, emb_dim):\n","    super().__init__()\n","    self.eps = 1e-5\n","    self.scale = nn.Parameter(torch.ones(emb_dim))\n","    self.shift = nn.Parameter(torch.zeros(emb_dim))\n","  def forward(self, x):\n","    mean = x.mean(dim=-1, keepdim=True)\n","    var = x.var(dim=-1, keepdim=True)\n","    x = (x - mean) / (torch.sqrt(var + self.eps))\n","    x = self.scale * x + self.shift\n","    return x\n","\n","class FeedForward(nn.Module):\n","  def __init__(self, emb_dim, drop_rate):\n","    super().__init__()\n","    self.net = nn.Sequential(\n","        nn.Linear(emb_dim, 4 * emb_dim),\n","        nn.GELU(),\n","        nn.Linear(4 * emb_dim, emb_dim),\n","        #nn.Dropout(drop_rate)\n","    )\n","  def forward(self, x):\n","      return self.net(x)\n","\n","\n","def generate_text_simple(model, idx, max_new_tokens, context_size):\n","    # idx is (batch, n_tokens) array of indices in the current context\n","    # how much to output. (max_new_token)\n","    for _ in range(max_new_tokens):\n","\n","        # Crop current context if it exceeds the supported context size\n","        # E.g., if LLM supports only 5 tokens, and the context size is 10\n","        # then only the last 5 tokens are used as context\n","        idx_cond = idx[:, -context_size:]\n","\n","        # Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # Focus only on the last time step\n","        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n","        logits = logits[:, -1, :]  # how much to output.\n","\n","        # Apply softmax to get probabilities\n","        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n","\n","        # Get the idx of the vocab entry with the highest probability value\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n","\n","        # Append sampled index to the running sequence\n","        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n","\n","    return idx\n","\n","\n","\n","torch.manual_seed(123)\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","batch = []\n","text1 = \"Every effort move you\"\n","text2 = \"Every day hold a\"\n","batch.append(torch.tensor(tokenizer.encode(text1)))\n","batch.append(torch.tensor(tokenizer.encode(text2)))\n","batch = torch.stack(batch, dim=0)\n","#print (batch.shape)\n","#print (batch)\n","\n","\n","model = GPTModel(GPT_CONFIG_124M)\n","model.eval()\n","#out = model(batch)\n","#print(\"Input batch:\\n\", batch)\n","#print(\"\\nOutput shape:\", out.shape)\n","#print(out)\n","\n","\n","start_text = \"Hello, I am\"\n","encoded = tokenizer.encode(start_text)\n","#print (\"encoded\", encoded)\n","##encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n","#print(\"encoded_tensor.shape\", encoded_tensor.shape)\n","#model.eval()\n","#out = generate_text_simple(model=model,idx=encoded_tensor,max_new_tokens=6,context_size=GPT_CONFIG_124M[\"context_length\"])\n","#print(\"Output:\", out)\n","#print(\"Output length:\", len(out[0]))\n","#decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n","#print(\"Decoded text:\", decoded_text)\n","\n","\n","def text_to_token_ids (text, tokenizer):\n","  encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n","  #encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","  return encoded_tensor\n","\n","def token_ids_to_text (token_ids, tokenizer):\n","  flat = token_ids.squeeze(0).tolist()\n","  decoded_text = tokenizer.decode(flat)\n","  return decoded_text\n","\n","start_context = \"Every effor moves you\"\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","model.eval()\n","token_ids = generate_text_simple(\n","    model=model,\n","    idx=text_to_token_ids(start_context, tokenizer),\n","    max_new_tokens=10,\n","    context_size=GPT_CONFIG_124M[\"context_length\"]\n",")\n","\n","print(\"Output text1111:\\n\",token_ids_to_text(token_ids, tokenizer))"],"metadata":{"id":"diluMPfpywhX","executionInfo":{"status":"ok","timestamp":1757591815820,"user_tz":240,"elapsed":1412,"user":{"displayName":"Manjit Singh","userId":"04641024576264730749"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c445e37f-b5d3-4619-b875-d0fe5f353af0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n","PyTorch verson: 2.8.0+cu126\n","Train loader:\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","Val loader:\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","torch.Size([2, 8]) torch.Size([2, 8])\n","288\n","33\n","Token Embedding Size torch.Size([2, 8, 256])\n","Position Embedding Size: torch.Size([8, 256])\n","Input Embedding Shape: torch.Size([2, 8, 256])\n","Input Shape is torch.Size([2, 8])\n","X shape is torch.Size([2, 8, 256])\n","keys shape is torch.Size([2, 8, 256])\n","X shape is torch.Size([2, 8, 256])\n","keys shape is torch.Size([2, 8, 256])\n","d_out 256\n","num_heads 2\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","d_out 256\n","num_heads 8\n","Output text1111:\n"," Hello, I am PERouthJr Od Punk AmazingRIPapolog berleans\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"lw7RlARfNKl-"},"execution_count":null,"outputs":[]}]}